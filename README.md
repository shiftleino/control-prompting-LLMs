# Controlling the Text Generated by GPT-2 using Prompting

This repository contains the [notebook](./control-prompting-mistral7b-instruct.ipynb) for conducting experiments on how the control signal for instructing Mistral 7B Instruct decays when additional context is added after the instruction before text generation. In the experiments, we will focus on the basic task of controlling the sentiment of the generated text. We demonstrate that the control signal in a sentiment control task decays exponentially as the number of tokens with opposite sentiment increases between the control prompt and the generated text. This result shows the inherent problem of decaying control when using prompting as the only approach for controlling text generation of LLMs. 

As the domain for the text generation we will utilize story generation. More comprehensive description of the experiments is provided in the [notebook](./control-prompting-mistral7b-instruct.ipynb) and the paper (tbd.).