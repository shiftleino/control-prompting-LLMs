{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decaying prompt control in text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrate with experiments how the control signal of instruction prompt decays as the distance increases between the prompt and the generated text. In the experiments, GPT-2 is first fine-tuned to generate continuations for stories according to the given sentiment in the control prompt. \n",
    "\n",
    "As the dataset for fine-tuning GPT-2 for story generation and for obtaining the story beginnings, we will be using the WritingPrompts dataset [1]. The dataset is originally collected from Reddit's [WritingPrompts forum](https://www.reddit.com/r/WritingPrompts/), where users can respond with stories to story prompts provided by other users. From this dataset, we will be using the stories written by the users to fine-tune GPT-2 to generate stories according to the sentiment given in the instruction. More specifically, in the fine-tuning and experiments stage we will utilize the following prompt: \"Continue the story with <|sentiment|> sentiment: <|story|>\", where <|sentiment|> is either \"positive\" or \"negative\" and <|story|> is a varying length start of a story from the WritingPrompts dataset.\n",
    "\n",
    "### Experiment setup\n",
    "Text Generation Model: [GPT-2 Medium](https://huggingface.co/openai-community/gpt2-medium) (355M parameters)<br>\n",
    "Sentiment Classifier Model: [Twitter-roBERTa-base for Sentiment Analysis](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) (125M parameters)<br>\n",
    "Dataset: [WritingPrompts](https://github.com/facebookresearch/fairseq/blob/main/examples/stories/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by first installing the needed libraries. We will use [Hugging Face](https://huggingface.co/) for downloading the pre-trained models, GPT-2 and fine-tuned RoBERTa, and [PyTorch](https://pytorch.org/) for fine-tuning GPT-2 to generate text according to the instructed sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GenerationConfig, RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "After importing the libraries, we will download the dataset and preprocess it for the fine-tuning and evaluation stages. To download the dataset, we follow the instructions given in the [README.md](https://github.com/facebookresearch/fairseq/blob/main/examples/stories/README.md) of the Hierarchical Neural Story Generation, which is the paper where the WritingPrompts dataset was presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0x writingPrompts/\n",
      "  0  363M    0  206k    0     0  1241k      0  0:04:59 --:--:--  0:04:59 1238k\n",
      "  3  363M    3 12.3M    0     0  5842k      0  0:01:03  0:00:02  0:01:01 5840k\n",
      "x writingPrompts/README\n",
      "x writingPrompts/valid.wp_source\n",
      "  8  363M    8 29.4M    0     0  7238k      0  0:00:51  0:00:04  0:00:47 7238k\n",
      " 94  363M   94  344M    0     0  6893k      0  0:00:54  0:00:51  0:00:03 6761kk  0  0:00:50  0:00:37  0:00:13 4908k\n",
      "100  363M  100  363M    0     0  6972k      0  0:00:53  0:00:53 --:--:-- 7689k\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0x writingPrompts/\n",
      "x writingPrompts/test.wp_source\n",
      "  3  363M    3 12.3M    0     0  7841k      0  0:00:47  0:00:01  0:00:46 7837k\n",
      "x writingPrompts/README\n",
      "x writingPrompts/valid.wp_source\n",
      "  8  363M    8 32.5M    0     0  9224k      0  0:00:40  0:00:03  0:00:37 9223k\n",
      " 95  363M   95  348M    0     0  10.0M      0  0:00:36  0:00:34  0:00:02 10.7M25k      0  0:00:39  0:00:07  0:00:32 9937k 0     0  9948k      0  0:00:37  0:00:20  0:00:17 10.5M0.5M\n",
      "100  363M  100  363M    0     0  10.0M      0  0:00:36  0:00:36 --:--:-- 10.8M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl https://dl.fbaipublicfiles.com/fairseq/data/writingPrompts.tar.gz | tar xvzf -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the experiments, we will use the target training dataset to fine-tune the GPT-2 model for generating stories, the validation dataset for checking overfitting during the training, and the test dataset for giving the initial context in the control signal evaluation phase. As we are only interested in the story generation capabilities of the model following the sentiment given in the first sentence of the story, we will only use the stories of the dataset (target) without the initial context prompts (source) for the stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename: str, encoding=\"utf-8\") -> list[str]:\n",
    "    with open(filename, \"r\", encoding=encoding) as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 272600\n",
      "Valid data: 15620\n",
      "Test data: 15138\n"
     ]
    }
   ],
   "source": [
    "train_data = load_file(\"writingPrompts/train.wp_target\")\n",
    "valid_data = load_file(\"writingPrompts/valid.wp_target\")\n",
    "test_data = load_file(\"writingPrompts/test.wp_target\")\n",
    "\n",
    "print(f\"Train data: {len(train_data)}\")\n",
    "print(f\"Valid data: {len(valid_data)}\")\n",
    "print(f\"Test data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In  what follows, we use the [Hugging Face Tokenizer](https://huggingface.co/docs/transformers/en/main_classes/tokenizer) for tokenizing the train, validation, and test datasets. As we use GPT-2 (medium) as our language model, we will use the GPT2TokenizerFast. Underneath the class abstractions, OpenAI models utilize the [BPE (Byte pair encoding) algorithm](https://en.wikipedia.org/wiki/Byte_pair_encoding) in tokenizing the input texts for its models. For GPT-2, this encoding has a vocabulary size of 50,257 tokens. Before tokenizing, we have to also replace the special token of `<newline>` in the datasets to the standard `\\n` token indicating a new line in the text. We will also add a random (to avoid overfitting) instruction from a pre-defined instruction list for the model to continue the story as during the test time we will include in this instruction the control prompt of the desired sentiment for the story. In addition, we will truncate all the stories including the instruction prompt to 512 tokens as the RoBERTa-based sentiment classifier can only classify sequences up to 512 tokens (different encoding algorithm but ballpark is the same), thus making it unnecessary to fine-tune the GPT-2 to generate longer stories. Shorter sequences than 512 are padded with the `<|pad|>` token. We will omit the usage of `<|endoftext|>` and `<|startoftext|>` tokens as during the training we won't concatenate multiple stories together, thus each batch consists of individual stories with their instruction prompts prepended.\n",
    "\n",
    "We will utilize a custom PyTorch DataSet class for constructing the training and validation datasets, and the PyTorch DataLoader for enabling the batching (batch_size=16) and shuffling (shuffle=True) required by the mini-batch stochastic gradient descent. As the fine-tuning dataset is quite large (272,600 stories), we will tokenize the stories only after they are selected to the current mini-batch. Therefore, the tokenization is done in a custom collate function of the DataLoader to enable tokenizing the whole batch in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryGenerationDataset(Dataset):\n",
    "    def __init__(self, data: list[str]):\n",
    "        self.data = data\n",
    "        self.instructions = [\n",
    "           \"Continue the story:\",\n",
    "           \"Keep the narrative going:\",\n",
    "           \"Resume the tale:\",\n",
    "           \"Carry on with the story:\",\n",
    "           \"Proceed with the plot:\",\n",
    "           \"Continue the narrative:\",\n",
    "           \"Move the story forward:\",\n",
    "           \"Keep telling the story:\",\n",
    "           \"Follow through with the story:\"\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        story = self.data[idx]\n",
    "        story = story.replace(\"<newline>\", \"\\n\").strip()\n",
    "        instruction = np.random.choice(self.instructions)\n",
    "        return f\"{instruction} {story}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch: list[str], tokenizer: GPT2TokenizerFast, max_length: int = 512):\n",
    "    encodings = tokenizer(text=batch, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(torch_device)\n",
    "    return encodings[\"input_ids\"], encodings[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", pad_token=\"<|endoftext|>\")\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = StoryGenerationDataset(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collate_batch(x, gpt2_tokenizer, max_length=512)\n",
    ")\n",
    "\n",
    "valid_dataset = StoryGenerationDataset(valid_data)\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collate_batch(x, gpt2_tokenizer, max_length=512)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our fine-tuning dataset and corresponding data loader ready, we can formulate the training loop for the fine-tuning process. For this we need to instantiate the GPT-2 Medium model object. We will use the Hugging Face model instantiation for this, openai-community/gpt2-medium. For the hyperparameters, we will use as the a starting learning rate `0.001` with a linearly decreasing schedule with warmup, warmup steps `100`, and epsilon `1e-8`. In total we will fine-tune for three epochs. At every 100 step we will generate a sample story text and also print out the validation loss, which is expected to decrease as the model adapts to the story domain. As the optimizer, we will use the AdamW-algorithm optimizer implemented by Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leinoj/dev/uni/ctg/controlled-story-generation/.venv/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-medium\").to(torch_device)\n",
    "gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "\n",
    "lr = 0.001\n",
    "eps = 1e-8\n",
    "num_epochs = 3\n",
    "num_warmup = 100\n",
    "\n",
    "optimizer = AdamW(gpt2_model.parameters(), lr=lr, eps=eps)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup, num_training_steps=total_steps)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=40,\n",
    "    pad_token_id=gpt2_tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "sample_input_prompt = \"Continue the story: The\"\n",
    "sample_input = gpt2_tokenizer(sample_input_prompt, return_tensors=\"pt\").to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    total_train_loss = 0\n",
    "    gpt2_model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids, attention_mask = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = gpt2_model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step} of {total_steps} - Loss: {loss.item()}\")\n",
    "\n",
    "            gpt2_model.eval()\n",
    "\n",
    "            sample_output = gpt2_model.generate(\n",
    "                inputs=sample_input[\"input_ids\"],\n",
    "                attention_mask=sample_input[\"attention_mask\"],\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "            sample_text = gpt2_tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "            print(f\"Sample output: {sample_text}\")\n",
    "            \n",
    "            gpt2_model.train()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(f\"Average training loss: {total_train_loss / len(train_dataloader)}\")\n",
    "\n",
    "    gpt2_model.eval()\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in valid_dataloader:\n",
    "        input_ids, attention_mask = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "    \n",
    "    print(f\"Average validation loss: {total_eval_loss / len(valid_dataloader)}\")\n",
    "        \n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\").to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] A. Fan, M. Lewis, and Y. Dauphin. Hierarchical neural story generation. In ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers), volume 1, page 889 â€“ 898, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
