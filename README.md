# Controlling the Text Generated by GPT-2 using Prompting

This repository contains the [notebook](./control-prompting-gpt2.ipynb) for conducting experiments on how the control signal for instructing GPT-2 decays when additional context is added after the instruction before text generation. In the experiments, we will focus on the basic task of controlling the sentiment of the generated text. As the domain for the text generation we will utilize story generation. More comprehensive description of the experiments is provided in the [notebook](./control-prompting-gpt2.ipynb).

In addition to the notebook containing the experiments, the repository contains also the lab report created based on the results and submitted for the course.